

export const secondEntry = {
    id: '2',
    title: 'Building a Langchain4j chatbot agent for banking customer service.',
    timestamp: '2024-06-27T10:30:00Z',
    headers: [
      'Application process',
      'Tech stack and general architecture', 
      'Teamwork: challenges and profits',
    ],
    paragraphs: [
      'Just after completing my work with the first team, summer was approaching, and the next project involved developing an internal chatbot: an AI-powered customer service platform for bank clients. Terms like "AI", "RAG", and "Vector database" frequently peppered our group chat. The initiative grew more intriguing with each passing day. True to their Swedish work culture, the team members had already meticulously planned their summer schedule, scheduling numerous meetings to discuss the tech stack and tools weeks before the project\'s kickoff.',
      'The tech stack was partly aligned with the more classical tools used across the different teams in the banking division, and included a backend built on Java Spring Boot, an integration layer (which I will explain in the following paragraph) and a react React webapp to fuel our frontend service. When it came to the more AI related engine, hosted in the integration layer, we used Langchain4j: a spinoff of the Python library “LangChain”. This library was included as a Spring dependency to our Spring boot repo, so that the installation and information management could be seamless. When it came to connecting the library to an LLM engine we filled our credit cards (two of us in the team) and bought a generous amount of tokens linked to our OPENAI API key. This API key not only allowed us to receive some good answers to natural language prompts, but also enabled us to translate strings (chunks of text) into meaning-vectors, which were then stored into a vector database the LLM was able to query. This was the RAG part, as you can guess. All these layers were hosted in Azure, where we deployed our three services: the backend, the integration layer, and the frontend. Azure served both as a great codebase as well as a versatile deployment tool. Pipelines and environments were quickly built with the help of some of the senior workers at the banking division and our Git policies were well defined, as well as our stable way of working. As you might imagine, the selection of tools and technologies did not happen in a couple of minutes: we started off trying the original Python library, and testing some free inference models via huggingface. The decisions were taken as a group, and were backed by simple PoC failures and a “get it done” mentality.',
      'Working with 15 other junior developers proved to be an incredibly enriching experience. We enjoyed full autonomy, reporting to our team leader only on Mondays and organizing ourselves based on availability and task preferences. I chose to focus on the integration layer, spending the early weeks conducting research and subsequently wrestling with our stubborn LLM engine. During the first two weeks, I worked alongside T. Strömberg, exploring creative approaches within the original Python framework. Our goal was to develop a stable agent capable of intelligently differentiating between calling its Tools, executing RAG queries, and responding based on its conversational training. The following week, J. Saeed joined the team with remarkable efficiency, decisively advocating for the Java library initially proposed. From my perspective, Java offers superior fail-traceability and type-class accountability. We were also more familiar with best practices for developing Java Spring Boot microservices, and ultimately, efficiency triumphed over our initial Tony Stark-esque experimental approach. It was fascinating to observe how a seemingly mechanical field like software engineering can be rife with human emotions—pride, impatience, frustration, and even jealousy. These sentiments were likely intensified by the collective decision to forgo summer vacations. Despite the challenges, I consider this project the most collaborative and transformative team experience of my career.', 
      'When it came to sharing knowledge, I highly appreciated the specific actions taken to spread knowledge. Knowledge in the sense of technical novelties, revising the structure of the code, or taking some time to explain at a high level how the architecture of the project looked. Knowledge could also be lifting up a problem one was facing that week to the seniors in the team, or the other way around: the architects in the team reminding us others how stuff had to be implemented. Another stable specific resource for sharing information about the project was the documentation and the internal network of documents that were revised in parallel to the development of certain stories. As you might imagine: it was a huge volume of data to assimilate, and still is. And this can, of course, either make you desperate and bored, or glad and joyful with an apparently endless fuel for your curiosity. Outside of the team, we had monthly meetings with all other teams across the teams working for other customers, and we even started a hobby-project group, where we shared features of new technology stacks and sat together and developed some simple proofs of concept (PoCs).', 
    ]
  };